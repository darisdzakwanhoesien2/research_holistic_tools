detected tone distortions to material outcomes, connect tone analysis to KPI-related sections, and validate indicators against real-world evidence—such as verified violations or regulatory actions. Expanding and replicating such analyses across languages, sectors, and time periods will further strengthen the robustness, comparability, and generalizability of findings [74,110,120].

Considering readability analysis as a tool to detect greenwashing in sustainability reporting, it is important to note that standard readability formulas are widely applied but often insufficiently calibrated to the linguistic and conceptual characteristics of ESG disclosures. Low readability may simply reflect the inclusion of necessary technical details rather than an attempt to confuse, while the excessive use of specialized jargon can sometimes be employed to obscure information [74,127,129,130]. Accordingly, readability metrics should be paired with domain-specific indicators—such as levels of specificity, baseline reporting, scope coverage, and verifiability—to better distinguish legitimate complexity from manipulative practices. Document-level readability averages can conceal localized tactics, as clusters of complexity often appear in particular sections—such as CEO letters, KPI or incident notes, and footnotes—precisely where decoupling or attention deflection is most likely to occur. Readability analysis therefore requires section- or span-level granularity linked to the KPIs discussed, in order to differentiate “accessible language on successes” from “opaque language on shortcomings.” While many studies relate readability to performance or disclosure style, fewer examine whether readability-based warning signals correspond to verified outcomes such as regulatory violations, enforcement actions, or financial restatements. Future research should leverage pre/post-event trajectories and matched control designs to test whether scrutiny actually disciplines corporate language or merely encourages greenhushing. Studies examining the readability of sustainability reporting frequently face methodological limitations, including a predominant focus on single-country or single-sector samples, reliance on voluntary disclosures, and susceptibility to coder subjectivity. These challenges highlight the need for cross-lingual, multi-year, and multi-industry datasets, transparent codebooks, and human-in-the-loop processes. It is also evident that readability interacts dynamically with tone, as well as with graphical and formatting choices, underscoring the need to develop multimodal analytical pipelines that integrate textual, visual, and claim-verification components [74,113,119].

Empirical evidence indicates that greenwashing frequently manifests in visual form. Companies can amplify positive outcomes and mute negative ones through the strategic design of charts, layouts, and imagery [80,81]. Yet empirical research often continues to treat visuals as illustrative artifacts rather than as measurable evidence, typically relying on small samples or broad impression-management categorizations [79]. Future studies should therefore advance chart-forensic methodologies that translate potentially misleading design choices into reproducible, quantifiable features and link these visual features to material KPIs (e.g. emissions, water use, accidents) and to verified outcomes such as violations, restatements, or enforcement actions [133]. At present, the available evidence still depends heavily on manual coding and limited samples, with no standardized chart-forensics feature set (e.g. baseline height, axis truncation, unit swaps, selective time windows, and 3D exaggeration). Visuals are also rarely connected to specific KPIs or materiality dimensions, even though greenwashing is often localized—appearing as a flattering chart in highlights, a footnoted base-year reset, or imagery that diverts attention. In addition, theoretical anchoring and causal inference remain weak: graphical manipulation is seldom linked to verified outcomes or exogenous shocks, leaving it unclear whether scrutiny disciplines visual design or merely prompts greenhushing. Moreover, replicable and open figure-level corpora and perception experiments that assess how design choices distort stakeholder judgments remain scarce. Accordingly, future studies should formalize and automate