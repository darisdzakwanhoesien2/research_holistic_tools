chart-forensic feature extraction, analyze visuals at the figure or section-level while linking them to KPIs and materiality, conduct quasi-experimental studies around assurance or regulatory interventions to connect visual-risk scores to verified outcomes, and establish open, replicable figure-level datasets complemented by human perception benchmarks.

Empirical studies that document actual greenwashing—drawing on enforcement lists, counter-accounts, scandals, or verified violations—have advanced the field beyond purely hypothetical typologies. Nevertheless, several cross-cutting research gaps remain. Research in this area is often event-bound or context-bound, which introduces selection bias and limits external validity. For example, reliance on media-identified cases carries the risk that “hidden” offenders remain unobserved [135]. Other limitations include small sample sizes, sector-specific scopes, and short temporal windows, underscoring the potential to scale counter-account methodologies using artificial intelligence and machine learning, and to extend analyses across more diverse sectors and longer time horizons [9,12]. Causal identification also remains underdeveloped, as most studies document associations rather than mechanisms. There is a growing call for clean empirical settings to test whether mandatory reporting regimes genuinely curb greenwashing rather than merely substituting for voluntary signaling. Quasi-experimental designs and broader contextual applications are needed to determine whether markets discipline actual misconduct or simply respond to disclosure salience [135,143]. Moreover, dynamic post-enforcement adaptations remain insufficiently measured. Regulatory scrutiny may prompt companies to shift from exaggeration to attention deflection or selective disclosure, yet systematic pre-/post-tracking is rare [113]. Finally, there is a pressing need for predictive and preventive tools capable of testing whether linguistic and visual features can anticipate future irresponsible behavior, alongside validated multi-metric pipelines—integrating readability, tone, and visual analysis—anchored to verified violations [113,131].

It should be noted that some studies exhibit a proxy--construct mismatch, inferring greenwashing merely from the issuance of a sustainability report [138], the adoption of reporting frameworks, or the presence of assurance statements without analyzing the actual report content [139]. Such approaches rely on surface-level or indirect indicators that are not validated against concrete evidence, which may conflate weak sustainability performance or incomplete disclosure with deceptive intent, thereby oversimplifying detection. Moreover, the absence of falsification tests, causal analysis, and cross-validation further undermines the reliability and robustness of these findings. Future research could address these limitations through systematic content analysis that explicitly examines omissions, vagueness, and materiality in disclosures, while refining the greenwashing construct with materiality-aware, section-level measures and triangulating proxies through multimodal evidence.

Table 5 presents avenues for advancing research on strategic typologies and measurement approaches for greenwashing in sustainability reporting.