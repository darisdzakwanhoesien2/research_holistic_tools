Sustainability 2026, 18, 17

Table 5. Cont.

|  Identified Research Gap | Research Questions | Suggested Research Approach  |
| --- | --- | --- |
|  Limited empirical validation and weak linkage to verified outcomes | Do readability-based indicators predict violations, enforcement actions, or restatements more effectively than ratings-based proxies? | Readability-event linkage studies; event studies; predictive validity testing; calibration by industry and technical thresholds  |
|  Limited understanding of post-event readability shifts; potential emphasis on immaterial topics or omission of material ones | After adverse events, do companies shift complex language toward immaterial topics or omit material ones? | Materiality mapping; topic-wise readability; missing-data diagnostics; pre/post event analysis  |
|  Limited understanding of cross-lingual, cross-country, cross-sector, and temporal readability effects | Are readability effects consistent across languages, countries, sectors, and time periods? | Multi-country, multi-language, and multi-sector panel analyses; rolling-window tests; robustness checks using alternative performance and assurance metrics  |
|  Limited understanding of readability-tone-visual interactions | Can multimodal pipelines combining text, tone, and visuals improve the detection of misleading sustainability claims? | Multimodal fusion models integrating readability, tone, and visual indicators; human adjudication with inter-rater reliability reporting  |
|  Avenue 7: Advancing visual analysis in sustainability reporting to detect greenwashing  |   |   |
|  Limited availability of a standardized, replicable chart-forensics toolkit for analyzing visual greenwashing | Which specific chart design choices (e.g., non-zero baselines, axis breaks, dual axes, 3-D effects, or selective time windows) systematically inflate perceptions of performance? | Develop a standardized feature schema (baseline, units, window, area/angle); CV/OCR detection; human validation with codebook and inter-rater reliability checks  |
|  Limited understanding of how local visual reporting tactics are obscured when using document-level | Do figure-level distortions appear in report sections associated with weaker or less material KPIs? | Conduct visual-KPI alignment (visuals, KPIs, periods, targets); compute visual-salience × KPI-weakness index; test section-level decoupling  |
|  Limited understanding of photos, icons, and color as “sustainable gloss” | Does the use of nature imagery, icons, and green colors increase when sustainability KPIs decline? | Apply image analysis; scene classification, icon detection, and color histograms; linked to KPI trends; attention-deflection test  |
|  Limited evidence on how selective windows, metric/unit switches, and dual-axis charts distort perceived performance | How frequently do charts omit adverse years, switch from absolute to intensity metrics, or use dual axes and unit swaps to exaggerate trends or correlations? | Assess window integrity (coverage vs. reporting boundary); detect metric swaps and base-year resets; peer-window comparison; dual-axis/unit detection; perceived slopes/ratios; exaggeration index.  |
|  Limited evidence on the effects of unclear labeling and visual distortions | Do labeling opacity and visual design jointly distort transparency and perceived magnitude in critical disclosure areas? | Perform layout forensics (font, contrast, placement, z-order) for legends/footnotes; link to boundary changes, geometry heuristics for data-ink distortion; perception tests vs. ground truth  |
|  Avenue 8: Toward a deeper understanding of real greenwashing evidence  |   |   |
|  Limited ability to generalize findings beyond specific events and contexts | Do results driven by headline cases hold in routine, low-salience settings and other contexts? | Construct multi-country panels of verified cases; implement systematic case verification and peer matching; perform cross-domain re-estimation to test robustness  |
|  Small samples, narrow sectoral focus, and restricted time frames | To what extent are earlier findings on greenwashing robust when examined across multiple sectors and over longer time horizons? | Apply a machine learning-augmented counter-accounting; use pre-registered codebooks; conduct sector-level stratification; rolling-window effect testing  |
|  Limited understanding of post-enforcement adaptation by companies | Do enforcement actions prompt companies to reduce explicit claims and shift toward greenhushing or less verifiable environmental narratives? | Employ event studies and hazard modeling of pre/post violations: perform missing-data diagnostics; longitudinal tracking of governance changes  |
|  Limited predictive detection and early warning of greenwashing | To what extent can textual and visual features predict future instances of irresponsible behavior or verified greenwashing? | Develop labeled corpora of verified cases; train predictive models using textual and visual features; perform stratified error evaluation and out-of-sample validation on new sustainability reports  |
|  Avenue 9: Advancing research to avoid oversimplified assumptions of greenwashing in sustainability reporting  |   |   |
|  Proxy-construct mismatch in greenwashing | When do sustainability reports or framework adoptions signal genuine performance rather than greenwashing? | Detailed content analysis; apply omission detection, vagueness assessment, materiality evaluation frameworks  |

https://doi.org/10.3390/su18010017